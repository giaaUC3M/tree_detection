{"cells":[{"cell_type":"markdown","metadata":{"id":"Z1eCfrEFeZT-"},"source":["# INSTALLS \u0026 IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9RDjBPbuNSsJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting DeepForest==0.3.8\n","  Downloading deepforest-0.3.8-cp37-cp37m-manylinux2010_x86_64.whl (9.3 MB)\n","\u001b[K     |████████████████████████████████| 9.3 MB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (1.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (1.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (7.1.2)\n","Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (1.15.0)\n","Collecting tensorflow==1.14.0\n","  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n","\u001b[K     |████████████████████████████████| 109.3 MB 44 kB/s \n","\u001b[?25hRequirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (3.38.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (4.1.2.30)\n","Collecting slidingwindow\n","  Downloading slidingwindow-0.0.14-py3-none-any.whl (9.0 kB)\n","Collecting xmltodict\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Collecting pyyaml\u003e5.1.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.2 MB/s \n","\u001b[?25hCollecting keras-resnet==0.1.0\n","  Downloading keras-resnet-0.1.0.tar.gz (6.8 kB)\n","Collecting keras==2.3.0\n","  Downloading Keras-2.3.0-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 47.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (4.64.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from DeepForest==0.3.8) (3.2.2)\n","Requirement already satisfied: numpy\u003e=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0-\u003eDeepForest==0.3.8) (1.21.6)\n","Requirement already satisfied: keras-preprocessing\u003e=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.0-\u003eDeepForest==0.3.8) (1.1.2)\n","Collecting keras-applications\u003e=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt\u003e=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (1.14.1)\n","Requirement already satisfied: grpcio\u003e=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (1.46.3)\n","Requirement already satisfied: astor\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (0.8.1)\n","Collecting tensorflow-estimator\u003c1.15.0rc0,\u003e=1.14.0rc0\n","  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n","\u001b[K     |████████████████████████████████| 488 kB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta\u003e=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (0.2.0)\n","Requirement already satisfied: protobuf\u003e=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (3.17.3)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (1.1.0)\n","Requirement already satisfied: gast\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (0.5.3)\n","Requirement already satisfied: absl-py\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (1.1.0)\n","Collecting tensorboard\u003c1.15.0,\u003e=1.14.0\n","  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 11.7 MB/s \n","\u001b[?25hRequirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0-\u003eDeepForest==0.3.8) (0.37.1)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (1.0.1)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (57.4.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (3.3.7)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (4.11.4)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (4.2.0)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0-\u003eDeepForest==0.3.8) (3.8.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003eDeepForest==0.3.8) (0.11.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003eDeepForest==0.3.8) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003eDeepForest==0.3.8) (3.0.9)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003eDeepForest==0.3.8) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eDeepForest==0.3.8) (2022.1)\n","Requirement already satisfied: python-utils\u003e=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2-\u003eDeepForest==0.3.8) (3.3.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from slidingwindow-\u003eDeepForest==0.3.8) (5.4.8)\n","Building wheels for collected packages: keras-resnet\n","  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-resnet: filename=keras_resnet-0.1.0-py2.py3-none-any.whl size=13343 sha256=14e7e529b519de7667ca8f54da3562cb314e6c79bf571656837892bc6eb7000a\n","  Stored in directory: /root/.cache/pip/wheels/a6/20/57/d7b7be8556e2ebf36345b3df4068a9a13bb90e4a2cc85a6994\n","Successfully built keras-resnet\n","Installing collected packages: h5py, pyyaml, keras-applications, tensorflow-estimator, tensorboard, keras, xmltodict, tensorflow, slidingwindow, keras-resnet, DeepForest\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.7 requires tensorflow\u003e=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n","Successfully installed DeepForest-0.3.8 h5py-2.10.0 keras-2.3.0 keras-applications-1.0.8 keras-resnet-0.1.0 pyyaml-6.0 slidingwindow-0.0.14 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 xmltodict-0.13.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-gpu==1.14.0\n","  Downloading tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1 MB)\n","\u001b[K     |████████████████████████████████| 377.1 MB 8.3 kB/s \n","\u001b[?25hRequirement already satisfied: protobuf\u003e=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (3.17.3)\n","Requirement already satisfied: keras-applications\u003e=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n","Requirement already satisfied: gast\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.5.3)\n","Requirement already satisfied: google-pasta\u003e=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n","Requirement already satisfied: grpcio\u003e=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.46.3)\n","Requirement already satisfied: tensorflow-estimator\u003c1.15.0rc0,\u003e=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n","Requirement already satisfied: numpy\u003c2.0,\u003e=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.21.6)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.37.1)\n","Requirement already satisfied: tensorboard\u003c1.15.0,\u003e=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n","Requirement already satisfied: wrapt\u003e=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.1)\n","Requirement already satisfied: astor\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n","Requirement already satisfied: absl-py\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n","Requirement already satisfied: keras-preprocessing\u003e=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n","Requirement already satisfied: six\u003e=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications\u003e=1.0.6-\u003etensorflow-gpu==1.14.0) (2.10.0)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (1.0.1)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (57.4.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (3.3.7)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (4.11.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (3.8.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow-gpu==1.14.0) (4.2.0)\n","Installing collected packages: tensorflow-gpu\n","Successfully installed tensorflow-gpu-1.14.0\n","Found existing installation: numpy 1.21.6\n","Uninstalling numpy-1.21.6:\n","  Would remove:\n","    /usr/bin/f2py\n","    /usr/local/bin/f2py\n","    /usr/local/bin/f2py3\n","    /usr/local/bin/f2py3.7\n","    /usr/local/lib/python3.7/dist-packages/numpy-1.21.6.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/numpy.libs/libgfortran-2e0d59d6.so.5.0.0\n","    /usr/local/lib/python3.7/dist-packages/numpy.libs/libopenblasp-r0-2d23e62b.3.17.so\n","    /usr/local/lib/python3.7/dist-packages/numpy.libs/libquadmath-2d0c479f.so.0.0.0\n","    /usr/local/lib/python3.7/dist-packages/numpy/*\n","Proceed (y/n)? "]}],"source":["#INSTALACION PAQUETE DEEPFOREST\n","!pip install DeepForest==0.3.8\n","!pip install tensorflow-gpu==1.14.0 # Para usar GPU\n","#es necesario desinstalar y reinstalar numpy por un problema de compatibilidad de versiones.\n","!pip uninstall numpy # PIDE METER UN YES OJO\n","!pip install numpy\n","# INSTALACIÓN PAQUETES TRATAR DATOS\n","!apt install gdal-bin python3-gdal\n","!apt-get install python-numpy python-scipy -y # Install numpy and scipy\n","!pip install pyshp # Shapefile\n","!pip install fiona==1.7\n","!pip install rasterio\n","!pip install pyproj\n","# INSTALACIÓN PAQUETES PARA ANALISIS RESULTADOS\n","!pip install plotly==4.14.3 # Instala en la máquina virtual Plot.ly\n","!pip install -U kaleido     # Instala la librería que exporta las figuras a imágenes\n","!pip install pyyaml==5.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4454,"status":"ok","timestamp":1655279754331,"user":{"displayName":"LAZARO FORNIS HERRANZ","userId":"12453935234529602211"},"user_tz":-120},"id":"fG_p-sCSN64k","outputId":"932b5e77-a82e-48ac-8860-01b2b0e8f781"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: fiona in /usr/local/lib/python3.7/dist-packages (1.8.21)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona) (2022.5.18.1)\n","Requirement already satisfied: click\u003e=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (7.1.2)\n","Requirement already satisfied: click-plugins\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.1.1)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona) (2.5.0)\n","Requirement already satisfied: six\u003e=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona) (57.4.0)\n","Requirement already satisfied: attrs\u003e=17 in /usr/local/lib/python3.7/dist-packages (from fiona) (21.4.0)\n","Requirement already satisfied: cligj\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona) (0.7.2)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: '/content/drive/MyDrive/tree_detection/Net_Trained_With_LiDAR_Detections'\n","/content\n"]}],"source":["# IMPORTS PRCESO DEEPFOREST\n","# ANTES HAY QUE DARLE A RESTART RUNTIME\n","import numpy as np\n","import os\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from deepforest import deepforest\n","from deepforest import get_data\n","from deepforest import utilities\n","from deepforest import preprocess\n","import pandas as pd\n","\n","# IMPORTS TRATAR CON DATOS\n","import gdal\n","import os\n","from osgeo import osr, ogr, gdal\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2\n","from skimage import io\n","from PIL import Image \n","import matplotlib.pylab as plt\n","from osgeo import ogr\n","from shapely.geometry import Polygon\n","import math\n","!pip install fiona\n","import fiona\n","from fiona.crs import from_epsg\n","from shapely.geometry import MultiPolygon, Polygon, mapping\n","import matplotlib.pyplot as plt\n","from descartes import PolygonPatch\n","import os\n","import rasterio\n","import rasterio.mask\n","import csv\n","#IMPORTS ANALISIS DE RESULTADOS\n","# Plotly\n","#from IPython.display import Image # to display images\n","import plotly       # Check that Plot.ly version is correct (prev versions than 4.14.3 do not use Kaleido)\n","plotly.__version__\n","import plotly.graph_objects as go # Se importa la librería\n","import plotly.express as px                         # Cosas de plotly generales. Como los colores\n","from plotly.offline import iplot, init_notebook_mode\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go              # Para generar las figuras y los charts\n","import plotly.io as pio                     # Interfaz para guardar en SVG\n","import math\n","from pyproj import Geod\n","from shapely import wkt\n","# Prepara los símbolos y colores\n","from plotly.validators.scatter.marker import SymbolValidator\n","\n","#IMPORTS ACCESO A COLAB\n","from google.colab import output # Permite limpiar el log de Colab cada X tiempo\n","from google.colab import drive # acceso a ficheros de Drive\n","from google.colab import files # descargar ficheros generados\n","import glob     # to access Google Drive files\n","import os       # tratar nombres de ficheros y carpetas\n","drive.mount('/content/drive')\n","# Definir rutas\n","path      = \"/content/drive/MyDrive/tree_detection/Net_Trained_With_LiDAR_Detections\"   # Base. Según la cuenta será una u otra.  \n","# Cambia a la carpeta\n","%cd {path}\n","#!ls                # Muestra los ficheros para comprobar que la carpeta es correcta\n","\n","tif_image      = 'Orthophoto_Colmenarejo_Train'               # Fichero original\n","fileName=tif_image\n","fileExtension  = '.tif'                 # Extensión del fichero"]},{"cell_type":"markdown","metadata":{"id":"XO8Bb9Qfeeif"},"source":["# Read datasets SHP LiDAR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqwowUTb35jm"},"outputs":[],"source":["#################\n","def world_to_pixel(geo_matrix, x, y):\n","    \"\"\"\n","    Uses a gdal geomatrix (gdal.GetGeoTransform()) to calculate\n","    the pixel location of a geospatial coordinate\n","    \"\"\"\n","    ul_x   = geo_matrix[0]\n","    ul_y   = geo_matrix[3]\n","    x_dist = geo_matrix[1]\n","    y_dist = geo_matrix[5]\n","    pixel  =  int((x - ul_x) / x_dist)\n","    line   = -int((ul_y - y) / y_dist)\n","    return pixel, line\n","\n","#################\n","def pixel_to_world(geo_matrix, x, y):\n","    ul_x = geo_matrix[0]\n","    ul_y = geo_matrix[3]\n","    x_dist = geo_matrix[1]\n","    y_dist = geo_matrix[5]\n","    _x = x * x_dist + ul_x\n","    _y = y * y_dist + ul_y\n","    return _x, _y\n","\n","#################\n","def build_transform_inverse(dataset, EPSG):\n","    source = osr.SpatialReference(wkt=dataset.GetProjection())\n","    target = osr.SpatialReference()\n","    target.ImportFromEPSG(EPSG)\n","    return osr.CoordinateTransformation(source, target)\n","\n","#################\n","def find_spatial_coordinate_from_pixel(dataset, transform, x, y):\n","    world_x, world_y = pixel_to_world(dataset.GetGeoTransform(), x, y)\n","    point = ogr.Geometry(ogr.wkbPoint)\n","    point.AddPoint(world_x, world_y)\n","    point.Transform(transform)\n","    return point.GetX(), point.GetY()\n","\n","#################\n","# La ortofoto tiene su lat-lon predefinido. \n","# Esta función recibe una Lat Lon esquina arriba izq y abajo der y te devuelve qué píxeles correponden a dichos lugares\n","def getTIFcornerPoints(fileName, LatAI, LonAI, LatAD, LonAD):\n","    fileExtension = \".tif\"\n","    dem = gdal.Open(fileName+fileExtension)\n","    xmin, res, rot1, ymax, px_h, rot2 = dem.GetGeoTransform() # get coordinates of upper left corner\n","\n","    # Extraer el EPSG que modela las coordenadas\n","    EPSG   = osr.SpatialReference(wkt=dem.GetProjection()).GetAttrValue('AUTHORITY',1)\n","    source = osr.SpatialReference()\n","    source.ImportFromEPSG(int(EPSG))\n","    # Transformación a WGS84\n","    wgs84 = osr.SpatialReference()\n","    wgs84.ImportFromEPSG(4326)\n","    transSourWGS = osr.CoordinateTransformation(source, wgs84)\n","    transWGSSour = osr.CoordinateTransformation(wgs84, source)\n","    # transformando los puntos de los recortes a hacer\n","    xAI, yAI, _ = transWGSSour.TransformPoint(LonAI, LatAI) # arriba izq v2\n","    xAD, yAD, _ = transWGSSour.TransformPoint(LonAD, LatAD)  # abajo der v2\n","\n","    #print(xAI, yAI, xAD, yAD)\n","    return xAI, yAI, xAD, yAD\n","\n","#################\n","def cornerPointsToSHP(outputName, xAI, yAI, xAD, yAD, EPSG=4326):\n","    finalPolygonList = []\n","    polygon = Polygon([(xAI, yAI),\n","                      (xAI, yAD),\n","                      (xAD, yAD),\n","                      (xAD, yAI),\n","                      (xAI, yAI)])\n","    finalPolygonList.append(polygon)\n","\n","    # Define a polygon feature geometry with one attribute\n","    polygonsToSHP(outputName, '.shp', finalPolygonList, EPSG=EPSG)\n","\n","def polygonsToSHP(nameOutputSHP, SHPExtension, finalPolygonList, EPSG=4326):\n","    schema = {\n","        'geometry': 'Polygon',\n","        'properties': {'id': 'int'},\n","    }\n","    # Se crea un fichero SHP en el que se almacenan los polygon\n","    outFile = fiona.open(nameOutputSHP+SHPExtension, mode = 'w', crs=from_epsg(EPSG), driver='ESRI Shapefile', schema=schema)\n","    for index, poly in enumerate(finalPolygonList):\n","        outFile.write({\n","            'geometry':mapping(poly),\n","            'properties': {\n","                'id': index\n","            },\n","        })\n","    outFile.close()\n","\n","####################\n","# Unir Shapefiles de detecciones LiDAR en uno solo\n","def mergeSHPs(nameOutputSHP, folderOrigSHP, SHPExtension=\".shp\"):\n","  # Open polygon layer and create list of with all the Polygons\n","  polyList = []\n","  polyProperties = []\n","\n","  for filename in os.listdir(folderOrigSHP):\n","    if filename.endswith(SHPExtension):\n","      #print(filename)\n","      polyShp = fiona.open(folderOrigSHP+\"/\"+filename)\n","      for poly in polyShp:\n","        polyGeom = poly['geometry']['coordinates'][0]\n","        polyList.append(polyGeom)\n","\n","  # Se crea la transformación a WGS84\n","  epsgSHP = polyShp.meta['crs']['init'] # Extraer el EPSG con el ultimo SHP (deberían ser todos iguales)\n","  epsgSHP = epsgSHP.replace(\"epsg:\", \"\")\n","  source = osr.SpatialReference()\n","  source.ImportFromEPSG(int(epsgSHP))\n","  EPSG_wgs84 = 4326\n","  wgs84 = osr.SpatialReference()\n","  wgs84.ImportFromEPSG(EPSG_wgs84)\n","  transSourWGS = osr.CoordinateTransformation(source, wgs84)\n","\n","  # Se cogen todos los polígonos de cada fichero\n","  finalPolygonList = []\n","  for poly in polyList:\n","    auxPoly = []\n","    for point in poly:\n","      Lon, Lat, _ = transSourWGS.TransformPoint(point[0],point[1])\n","      a = (Lon, Lat)\n","      auxPoly.append(a)\n","    # Se genera un polígono (que cierra) del árbol detectado en WGS84\n","    polygon = Polygon(auxPoly)\n","    finalPolygonList.append(polygon)\n","\n","  # Export clipped polygons as shapefile\n","  polygonsToSHP(nameOutputSHP, SHPExtension, finalPolygonList, EPSG=EPSG_wgs84)\n","\n","#################\n","# \n","def TIFgetESPG(fileName):\n","  # Abro la imagen y extraigo su EPSG\n","  fileExtension = \".tif\"\n","  dem = gdal.Open(fileName+fileExtension)\n","  # Extraer el EPSG que modela las coordenadas\n","  EPSG   = osr.SpatialReference(wkt=dem.GetProjection()).GetAttrValue('AUTHORITY',1)\n","  dem   = None\n","  return int(EPSG)\n","\n","#################\n","def SHPgetESPG(SHPName, SHPExtension=\".shp\"):\n","  polyShp = fiona.open(SHPName+SHPExtension)\n","  epsgSHP = polyShp.meta['crs']['init']\n","  epsgSHP = epsgSHP.replace(\"epsg:\", \"\")\n","  return int(epsgSHP)\n","\n","#################\n","def cropTIFbySHP(TIFOriginalName, TIFOutputName, SHPName):\n","  EPSG = TIFgetESPG(TIFOriginalName)\n","  TIFExtension = \".tif\"\n","  SHPExtension = \".shp\"\n","\n","  # Cargo el SHP que define la zona a cortar\n","  with fiona.open(SHPName+SHPExtension, \"r\") as shapefile:\n","    shapes = [feature[\"geometry\"] for feature in shapefile]\n","\n","  # Abro la imagen y la corto con lo definido\n","  with rasterio.open(TIFOriginalName+TIFExtension) as src:\n","    out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n","    out_meta = src.meta\n","    a = 'EPSG:'+str(EPSG)\n","    out_meta.update({\n","      \"driver\": \"GTiff\",\n","      \"height\": out_image.shape[1],\n","      \"width\": out_image.shape[2],\n","      \"transform\": out_transform,\n","      \"crs\": a\n","    })\n","\n","  # Guardo el corte\n","  with rasterio.open(TIFOutputName+TIFExtension, \"w\", **out_meta) as dest:\n","    dest.write(out_image)\n","\n","#################\n","def findMinMaxLatLonSHP(SHPName, SHPExtension=\".shp\"):\n","  # Identificar LatLon esquinas de todos los Shapefile, para definir el area del recorte a realizar\n","  minLat = 999\n","  minLon = 999\n","  maxLat =-999\n","  maxLon =-999\n","\n","  polyShp = fiona.open(SHPName)\n","  for poly in polyShp:\n","    polyGeom = poly['geometry']['coordinates'][0]\n","    for point in polyGeom:\n","      if minLat \u003e point[1]:\n","        minLat = point[1]\n","      if point[1] \u003e maxLat:\n","        maxLat = point[1]\n","      if minLon \u003e point[0]:\n","        minLon = point[0]\n","      if point[0] \u003e maxLon:\n","        maxLon = point[0]\n","\n","  return minLat, minLon, maxLat, maxLon\n","\n","#################\n","def TIFToPNG(filename):\n","  TIFExtension = \".tif\"\n","  PNGExtension = \".png\"\n","  img = cv2.imread(filename+TIFExtension)\n","  cv2.imwrite(filename+PNGExtension, img)\n","\n","#################\n","def XYPolygonToCSV(CSVName, TIFName, polyList, TIFExtension=\".tif\", CSVExtension=\".csv\"):\n","  with open(CSVName+CSVExtension, mode='w') as csv_write:\n","    csv_write = csv.writer(csv_write, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","    csv_write.writerow(['image_path','xmin','ymin','xmax','ymax','label']) # Header\n","    for index, poly in enumerate(polyList):\n","      xAI = poly[0]\n","      yAI = poly[1]\n","      xAD = poly[2]\n","      yAD = poly[3]\n","\n","      # Los invierte para que esten bien colocadas las BB\n","      if xAI \u003e xAD:\n","        a = xAD\n","        b = xAI\n","        xAD = b\n","        xAI = a\n","      if yAI \u003e yAD:\n","        a = yAD\n","        b = yAI\n","        yAD = b\n","        yAI = a\n","      csv_write.writerow([TIFName+TIFExtension, xAI, yAI, xAD, yAD, 'Tree'])\n","\n","#################\n","def BBtoTIFpixel(polyList, SHPEPSG, TIFName, CSVName, TIFExtension=\".tif\", CSVExtension=\".csv\"):\n","  # Abro la foto en la que quiero extraer las posiciones\n","  dem = gdal.Open(TIFName+TIFExtension)\n","  EPSG = TIFgetESPG(TIFName)\n","  print(\"Imagen EPSG: \", str(EPSG))\n","\n","  source = osr.SpatialReference()\n","  source.ImportFromEPSG(int(EPSG))\n","  EPSG_wgs84 = SHPEPSG\n","  wgs84 = osr.SpatialReference()\n","  wgs84.ImportFromEPSG(EPSG_wgs84)\n","  transSourWGS = osr.CoordinateTransformation(source, wgs84)\n","  transWGSSour = osr.CoordinateTransformation(wgs84, source)\n","\n","  # Cada detección a pixel\n","  newPolyList = []\n","  for index, poly in enumerate(polyList): # Cada poligono\n","    #print(poly)\n","    newPolyX = []\n","    newPolyY = []\n","    for polyVertex in poly:             # cada vértice del polígono\n","      #print(polyVertex)\n","      # Lo transformo a las EPSG de la TIF\n","      xTIF, yTIF, _ = transWGSSour.TransformPoint(polyVertex[0], polyVertex[1])\n","      #print(xTIF, yTIF)\n","      # Hace lo mismo que lo de arriba\n","      #with rasterio.open(TIFName+TIFExtension) as src:\n","      #    rows, cols = rasterio.transform.rowcol(src.transform, x, y)\n","      # Y finalmente a Lat, Lon\n","      x, y = world_to_pixel(dem.GetGeoTransform(), xTIF, yTIF)\n","      #print(x, y)\n","      newPolyX.append(x)\n","      newPolyY.append(y)\n","\n","    newPolyX = list(set(newPolyX)) # Pilla los unique\n","    newPolyY = list(set(newPolyY))\n","\n","    if len(newPolyX) == 1: # Si solo hay 1, es que esta mal la BB\n","      continue\n","    if len(newPolyY) == 1:\n","      continue\n","\n","    # Los coloca correctamente para una BB\n","    if newPolyX[0] \u003e newPolyX[1]:\n","      xAI = newPolyX[1]\n","      xAD = newPolyX[0]\n","    else:\n","      xAI = newPolyX[0]\n","      xAD = newPolyX[1]\n","    if newPolyY[0] \u003e newPolyY[1]:\n","      yAI = newPolyY[1]\n","      yAD = newPolyY[0]\n","    else:\n","      yAI = newPolyY[0]\n","      yAD = newPolyY[1]\n","\n","    newPolyList.append([xAI, yAI, xAD, yAD])\n","\n","  return newPolyList\n","\n","#################\n","def SHPtoCSV(SHPName, TIFName, CSVName, SHPExtension=\".shp\", TIFExtension=\".tif\", CSVExtension=\".csv\"):\n","  print(\"SHPtoCSV\")\n","  #open polygon layer and create list of Polygons\n","  SHP_ESPG = SHPgetESPG(SHPName, SHPExtension)\n","  polyList = SHPgetPolygons(SHPName, SHPExtension)\n","  print(len(polyList))\n","  print(\"SHP EPSG: \", str(SHP_ESPG))\n","  # Pasa los poligonos a XY de la imagen dada\n","  newPolyList = BBtoTIFpixel(polyList, SHP_ESPG, TIFName, CSVName, TIFExtension, CSVExtension)\n","  # Escritura a CSV¿?\n","  print(len(newPolyList))\n","  XYPolygonToCSV(CSVName, TIFName, newPolyList, TIFExtension, CSVExtension)\n","\n","###################\n","def deepForestToSHP(trained_model_boxes, TIFName, SHPOutputName, SHPExtension=\".shp\", TIFExtension=\".tif\"):\n","  # Se transforman a WGS84\n","  EPSG_wgs84 = 4326\n","  ds = gdal.Open(TIFName+TIFExtension)\n","  _t = build_transform_inverse(ds, EPSG_wgs84) # WGS84\n","\n","  # Generacion de los poligonos \n","  finalPolygonList = []\n","  for index, BB in trained_model_boxes.iterrows():\n","    auxPoly = []\n","    xAI = BB[\"xmin\"]\n","    xAD = BB[\"xmax\"]\n","    yAI = BB[\"ymin\"]\n","    yAD = BB[\"ymax\"]\n","\n","    coordinates = find_spatial_coordinate_from_pixel(ds, _t, xAI,yAI)\n","    auxPoly.append( coordinates )\n","    coordinates = find_spatial_coordinate_from_pixel(ds, _t, xAI,yAD)\n","    auxPoly.append( coordinates )\n","    coordinates = find_spatial_coordinate_from_pixel(ds, _t, xAD,yAD)\n","    auxPoly.append( coordinates )\n","    coordinates = find_spatial_coordinate_from_pixel(ds, _t, xAD,yAI)\n","    auxPoly.append( coordinates )\n","    coordinates = find_spatial_coordinate_from_pixel(ds, _t, xAI,yAI)\n","    auxPoly.append( coordinates )\n","    # Se genera un polígono (que cierra) del árbol detectado en WGS84\n","    polygon = Polygon(auxPoly)\n","    finalPolygonList.append(polygon)\n","\n","  # Export clipped polygons as shapefile\n","  polygonsToSHP(SHPOutputName, SHPExtension, finalPolygonList, EPSG=EPSG_wgs84)"]},{"cell_type":"markdown","metadata":{"id":"JDBQZvS1hyn1"},"source":["# Merge SHP LiDAR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZeRZhFBhyTW"},"outputs":[],"source":["# JUNTA TODOS LOS SHP LIDAR (LOS ARBOLES DETECTADOS DE R) EN UN UNICO SHP PARA TENERLOS TODOS JUNTOS\n","# USO COLMENOUC3M PORQUE EN ALGUNA CARPETA ESTAN TODOS LOS SHP MENOS EL DEL CAMPUS\n","\n","# Unión SHP SIN la zona de la UC3M\n","mergeSHPs(\"Train/Train\", \"LiDAR_Detections/LiDAR_Detections_for_Training/\")\n","\n","# Unión SHP CON la zona de la UC3M\n","mergeSHPs(\"Example/Example\", \"LiDAR_Detections/LiDAR_Detections_for_Example/\")"]},{"cell_type":"markdown","metadata":{"id":"3wThOKZZDtCx"},"source":["# Clean LiDAR detections with OSM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2_3j5ASt3xD"},"outputs":[],"source":["###########\n","# Limpieza de SHP si coinciden un porcentaje con otros SHP (edificios, carreteras...)\n","# Apunta los que eliminaría en un CSV, porque tarda mucho y lo mismo peta\n","###########\n","def cleanSHPOverlappingRoadCSV(cleanSHP, cleanPath, roadSHP, roadPath, SHPExtension=\".shp\", threshold=25):\n","  # Abre ambos SHP\n","  polygonsToClean = SHPgetPolygons(cleanPath+cleanSHP, SHPExtension)\n","  polygonsRoads   = SHPgetPolygons(roadPath+roadSHP, SHPExtension)\n","  polygonsRoads = polygonsRoads[0]\n","  cleanEPSG = SHPgetESPG(cleanPath+cleanSHP)\n","  roadEPSG  = SHPgetESPG(roadPath+roadSHP)\n","\n","  # Crea un fichero donde va a apuntando por orden cuales elimina\n","  f = open(cleanPath+\"DeletedRoadsDetections.txt\", \"a\")\n","  for idx, polyClean in enumerate(polygonsToClean): # Por cada poligono a limpiar\n","    if idx \u003c= 0:     # Para continuar desde donde terminó. COLOCARLO A MANO\n","      continue\n","    if idx%500 == 0:  # Para limpiar un poco el output del Colab y que no vaya como la mierda\n","      output.clear()\n","      f.close()       # Cierra para que se guarde en el CSV cada cierto tiempo\n","      f = open(cleanPath+\"DeletedRoadsDetections.txt\", \"a\")\n","    if idx%10 == 0:   # Para imprimir menos\n","      print(idx, \"de\", len(polygonsToClean))\n","\n","    checkAllOSM = True\n","    p1 = polygonToWGS84(polyClean, cleanEPSG) # Se transforma de array y como estuviese, a WGS84 y a Polygon\n","    for polyRoads in polygonsRoads:           # Por cada carretera [0]\n","      p2 = polygonToWGS84(polyRoads[0], roadEPSG)\n","      overlapDetected   = getOverlapPercentage(p1, p2) # Interseca con la carretera\n","      if overlapDetected \u003e threshold: # Si supera un threshold, este polígono al carrer\n","        intersectsRoad = True\n","        for idx2 in range(1,len(polyRoads)): # Se mira el resto de huecos del poligono de la carretera\n","          # se hace la intersección de la detección a este hueco\n","          p3 = polygonToWGS84(polyRoads[idx2], roadEPSG)\n","          overlapDetected2 = getOverlapPercentage(p1, p3) # Interseca con la carretera\n","          if overlapDetected2 \u003e threshold: # Si supera un threshold, está más dentro del hueco que de la carretera, luego no se elimina\n","            intersectsRoad = False\n","            break\n","        # Al mirar todos los huecos de esta carretera, sigue dentro, se elimina\n","        if intersectsRoad == True:\n","          print(\"hecho a: \", idx)\n","          f.write(str(idx)+\"\\n\")\n","\n","###########\n","# Limpieza de SHP si coinciden un porcentaje con otros SHP (edificios, carreteras...)\n","# Escribe los polígonos MENOS LOS DEL CSV en un nuevo SHP\n","###########\n","def cleanSHPCSVtoSHP(cleanSHP, cleanPath, inputCSV, outputName, SHPExtension=\".shp\"):\n","  # Abre ambos SHP\n","  polygonsToClean = SHPgetPolygons(cleanPath+cleanSHP, SHPExtension)\n","  cleanEPSG       = SHPgetESPG(cleanPath+cleanSHP)\n","  # Lectura del txt\n","  array = []\n","  with open(cleanPath+inputCSV) as f:\n","    for line in f:\n","      #print(line)\n","      array.append(int(line))\n","  # Pilla todos los polygons\n","  newPolyList = []\n","  for idx, polyClean in enumerate(polygonsToClean): # Por cada poligono a limpiar\n","    if idx%5000 == 0:  # Para limpiar un poco el output del Colab y que no vaya como la mierda\n","      output.clear()\n","    if idx%500 == 0:   # Para imprimir menos\n","      print(idx, \"de\", len(polygonsToClean))\n","    p1 = polygonToWGS84(polyClean, cleanEPSG) # Se transforma de array y como estuviese, a WGS84 y a Polygon\n","    if not idx in array: # Si no ha entrado petado con ninguno, se escribe, no overlapea\n","      newPolyList.append(p1)\n","  # Escribe los polígonos almacenados en un SHP\n","  polygonsToSHP(cleanPath+outputName, SHPExtension, newPolyList, EPSG=4326)\n","\n","###########\n","# Limpieza de SHP si coinciden un porcentaje con otros SHP (edificios, carreteras...)\n","# Apunta los que eliminaría en un CSV, porque tarda mucho y lo mismo peta\n","###########\n","def cleanSHPOverlappingBuildingCSV(cleanSHP, cleanPath, buildingsSHP, buildingsPath, SHPExtension=\".shp\", threshold=25):\n","  # Abre ambos SHP\n","  polygonsToClean   = SHPgetPolygons(cleanPath+cleanSHP, SHPExtension)\n","  polygonsBuildings = SHPgetPolygons(buildingsPath+buildingsSHP, SHPExtension)\n","  cleanEPSG     = SHPgetESPG(cleanPath+cleanSHP)\n","  buildingsEPSG = SHPgetESPG(buildingsPath+buildingsSHP)\n","\n","  # Crea un fichero donde va a apuntando por orden cuales elimina\n","  f = open(cleanPath+\"DeletedBuildingsDetections.txt\", \"a\")\n","  for idx, polyClean in enumerate(polygonsToClean): # Por cada poligono a limpiar\n","    if idx \u003c= 0:     # Para continuar desde donde terminó. COLOCARLO A MANO\n","      continue\n","    if idx%500 == 0:  # Para limpiar un poco el output del Colab y que no vaya como la mierda\n","      output.clear()\n","      f.close()       # Cierra para que se guarde en el CSV cada cierto tiempo\n","      f = open(cleanPath+\"DeletedBuildingsDetections.txt\", \"a\")\n","    if idx%10 == 0:   # Para imprimir menos\n","      print(idx, \"de\", len(polygonsToClean))\n","\n","    p1 = polygonToWGS84(polyClean, cleanEPSG) # Se transforma de array y como estuviese, a WGS84 y a Polygon\n","    for polyBuildings in polygonsBuildings:         # Por cada edificio\n","      p2 = polygonToWGS84(polyBuildings, buildingsEPSG)\n","      overlapDetected   = getOverlapPercentage(p1, p2) # Se calcula el porcentaje de overlap\n","      if overlapDetected \u003e threshold: # Si supera un threshold, este polígono al carrer\n","        f.write(str(idx)+\"\\n\")\n","        break\n","  \n","###########\n","# Calcula el porcentaje de overlap de un polígono respecto a otro\n","###########\n","def getOverlapPercentage(p1, p2):\n","  overlapDetected   = p2.intersection(p1).area / p1.area * 100\n","  return overlapDetected\n","\n","###########\n","# Transforma un array a un polígono en un EPSG determinado\n","###########\n","def polygonToWGS84(originArray, originEPSG, destEPSG=4326):\n","  if originEPSG == destEPSG:\n","    transformed = []\n","    for point in originArray:\n","      transformed.append([point[0], point[1]])\n","  else:\n","    # Extraer el EPSG que modela las coordenadas\n","    source = osr.SpatialReference()\n","    source.ImportFromEPSG(int(originEPSG))\n","    # Transformación a WGS84\n","    wgs84 = osr.SpatialReference()\n","    wgs84.ImportFromEPSG(destEPSG)\n","    transSourWGS = osr.CoordinateTransformation(source, wgs84)\n","    transWGSSour = osr.CoordinateTransformation(wgs84, source)\n","    \n","    # transformando los puntos de los recortes a hacer\n","    transformed = []\n","    for point in originArray:\n","      xAI, yAI, _ = transSourWGS.TransformPoint(point[0], point[1]) # arriba izq v2\n","      transformed.append([xAI, yAI])\n","  b = Polygon(transformed)\n","  return b\n","\n","###########\n","# \n","###########\n","def SHPgetPolygons(SHPName, SHPExtension=\".shp\"):\n","  polyList = []\n","  with fiona.open(SHPName+SHPExtension) as polyShp:\n","    if len(polyShp) == 1:  # Es multiPolygon\n","      #print(\"Es multipolygon\")\n","      multiPol = polyShp[0]['geometry']['coordinates']\n","      #print(\"multiPol\", len(multiPol))\n","      newPoly = []\n","\n","      for b in multiPol:\n","        #print(\"b\", len(b))\n","        newPoly2 = []\n","        for c in b:\n","          #print(\"c\", len(c))\n","          newPoly2.append( c )\n","          #for d in c:  # Aqui ya siempre son 2, tuplas XY\n","          #  print(\"d\", len(d))   \n","        newPoly.append( newPoly2 )\n","      polyList.append( newPoly )\n","\n","    else: # Es polygon\n","      for poly in polyShp:\n","        polyGeom = poly['geometry']['coordinates']\n","        polyList.append(polyGeom[0])\n","      #return polyList\n","  \n","  return polyList"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN0KLqI9ENhg"},"outputs":[],"source":["cleanSHPOverlappingBuildingCSV(\"Train\", \"Train/\", \"BuildingsWGS84\", \"To_Clean_LiDAR_Detections/\")\n","cleanSHPCSVtoSHP(\"Train\", \"Train/\", \"DeletedBuildingsDetections.txt\", \"Train_NoBuildings\")\n","\n","cleanSHPOverlappingRoadCSV(\"Train_NoBuildings\", \"Train/\", \"RoadsEPSG3345\", \"To_Clean_LiDAR_Detections/\")\n","cleanSHPCSVtoSHP(\"Train_NoBuildings\", \"Train/\", \"DeletedRoadsDetections.txt\", \"Train_NoBuildings_NoRoads\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIGpmrW5YH4P"},"outputs":[],"source":["cleanSHPOverlappingBuildingCSV(\"Example\", \"Example/\", \"BuildingsWGS84\", \"To_Clean_LiDAR_Detections/\")\n","cleanSHPCSVtoSHP(\"Example\", \"Example/\", \"DeletedBuildingsDetections.txt\", \"Example_NoBuildings\")\n","\n","cleanSHPOverlappingRoadCSV(\"Example_NoBuildings\", \"Example/\", \"RoadsEPSG3345\", \"To_Clean_LiDAR_Detections/\")\n","cleanSHPCSVtoSHP(\"Example_NoBuildings\", \"Example/\", \"DeletedRoadsDetections.txt\", \"Example_NoBuildings_NoRoads\")"]},{"cell_type":"markdown","metadata":{"id":"ZQzIDxJwhn10"},"source":["# SHP LiDAR detections related in a CSV with each pixel of the TIF image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C51gHgLYmvMm"},"outputs":[],"source":["# TENEMOS UN SHP CON MILES DE RECTANGULOS EN LAT LON (EN UN SISTEMA DE COORDENDAS CONCRETO. WGS84??).\n","# LOS CONVERTIMOS A PIXELES EN LA ORTOFOTO RECORTADA DE COLME\n","# Pasamos las detecciones del LiDAR a un CSV para entrenar la ortofoto seleccionada\n","#EJ. PARA EL ARBOL EN LAT LONG X, LE CORRESPONDE EN LA IMAGEN DADA EL PIXEL Y\n","csv_train = \"Train/Train_CSV_annotations/CSV_ALL_ORTHOPHOTO_annotations\"\n","SHPtoCSV(\"Train/Train_NoBuildings_NoRoads\", tif_image, csv_train)\n"]},{"cell_type":"markdown","metadata":{"id":"atPtDvpkD7Ge"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYM6pRXc1O2H"},"outputs":[],"source":["###################\n","def cropsDeepForest(TIFName, CSVName, trainTest, nameOutputModel, TIFExtension=\".tif\", CSVExtension=\".csv\", modelExtension=\".h5\"):\n","    # Lectura del CSV original\n","    dataset_annotations = pd.read_csv(CSVName+CSVExtension)\n","    # División de la imagen completa en trozos de 400x400 para entrenar\n","    cropsFolder = 'Train/Train_CSV_annotations/CROPS,NET_INPUTS'\n","    print(TIFName+TIFExtension)\n","    train_annotations = preprocess.split_raster(CSVName+CSVExtension, path_to_raster=TIFName+TIFExtension, \n","                                                patch_size=400, base_dir=cropsFolder, patch_overlap=0.05)\n","    print(\"División en imágenes 400x400 completada\")\n","    return train_annotations\n","\n","def trainDeepForest(train_annotations, CSVName, trainTest, nameOutputModel, TIFExtension=\".tif\", CSVExtension=\".csv\", modelExtension=\".h5\"):\n","    # División del CSV completo en Train-Test para que aprenda la red\n","    image_dir = train_annotations.image_path.unique()\n","    print(image_dir)\n","    test_dir  = np.random.choice(image_dir, trainTest[1]) # Porcentaje test\n","    test_annotations  = train_annotations.loc[ train_annotations.image_path.isin(test_dir)]\n","    train_annotations = train_annotations.loc[~train_annotations.image_path.isin(test_dir)]\n","    #train_annotations.head() # impresion principio tabla\n","    #print(\"There are {} training crown annotations\".format(train_annotations.shape[0]))\n","    #print(\"There are {} test crown annotations\".format(test_annotations.shape[0]))\n","    print(\"División CSV train-test completada\")\n","\n","    cropsFolder = 'Train/Train_CSV_annotations/CROPS,NET_INPUTS/'\n","    # Genera ambos CSV\n","    filenameTrain = cropsFolder+\"train\"+nameOutputModel+CSVExtension\n","    filenameTest  = cropsFolder+\"test\"+nameOutputModel+CSVExtension\n","    train_annotations.to_csv(filenameTrain, index=False, header=False)\n","    test_annotations.to_csv( filenameTest,  index=False, header=False)\n","\n","    ##### Creacion del modelo\n","    model = deepforest.deepforest() # Crea un modelo nuevo\n","    model.config[\"gpus\"] = 1 # Para utilizar la GPU de Google Colab (activarla first)\n","    model.config[\"epochs\"] = 5\n","    model.config[\"save-snapshot\"] = True\n","    print(\"A generar modelo\")\n","    model.train(annotations=filenameTrain, input_type=\"fit_generator\") #ENTRENAMIENTO MODELO CON CONJUNTO DE ENTRENAMIENTO\n","    print(\"Modelo entrenado, guardando\")\n","    #model.evaluate_generator(filenameTrain) #EVALUACIÓN DEL NUEVO MODELO MODELO: ACIERTO 60%\n","    model.model.save('Model/'+nameOutputModel+modelExtension) #GUARDAMOS EL MODELO\n","\n","# guardar las anotaciones en CSV\n","def annotationsToCSV(name, train_annotations):\n","  train_annotations.to_csv(name+'_annotations.csv')\n","\n","# cargar las anotaciones en CSV en el mismo objeto Pandas Dataframe\n","def loadCSVannotations(name):\n","  aa = pd.read_csv(name+'_annotations.csv')\n","  aa = aa.drop(aa.columns[[0]], axis=1)\n","  return aa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":250,"status":"error","timestamp":1655279641327,"user":{"displayName":"LAZARO FORNIS HERRANZ","userId":"12453935234529602211"},"user_tz":-120},"id":"p3rpEjk6dPyY","outputId":"e7263ddd-a58d-4aa2-ab67-54021a4ea148"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-12-3bcfc6eea9e3\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhaveCSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhaveCSV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Carga las anotaciones directamente de un CSV, en vez de calcularlas y hacer todos los crop de nuevo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m   \u001b[0mtrain_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadCSVannotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train/Train_CSV_annotations/CSV_ORTHOPHOTO_CROPS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_IMAGE_PIXELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m337351030\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-6-f1aedee2e983\u003e\u001b[0m in \u001b[0;36mloadCSVannotations\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# cargar las anotaciones en CSV en el mismo objeto Pandas Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadCSVannotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m   \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_annotations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--\u003e 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train/Train_CSV_annotations/CSV_ORTHOPHOTO_CROPS_annotations.csv'"]}],"source":["# Entrenando campusNotBad\n","modelName='model'\n","#haveCSV = False\n","haveCSV = True\n","if haveCSV == True: # Carga las anotaciones directamente de un CSV, en vez de calcularlas y hacer todos los crop de nuevo\n","  train_annotations = loadCSVannotations('Train/Train_CSV_annotations/CSV_ORTHOPHOTO_CROPS')\n","else:\n","  Image.MAX_IMAGE_PIXELS = 337351030\n","  train_annotations = cropsDeepForest(tif_image, 'Train/Train_CSV_annotations/CSV_ALL_ORTHOPHOTO_annotations', [70, 30], modelName)\n","  annotationsToCSV('Train/Train_CSV_annotations/CSV_ORTHOPHOTO_CROPS', train_annotations) # Guarda las anotaciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMg7uoNP-HQO"},"outputs":[],"source":["#print(train_annotations.image_path)\n","trainDeepForest(train_annotations, csv_train, [70, 30], modelName)"]},{"cell_type":"markdown","metadata":{"id":"t2AWXL0lRuZM"},"source":["# Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":537},"executionInfo":{"elapsed":386,"status":"error","timestamp":1655279612754,"user":{"displayName":"LAZARO FORNIS HERRANZ","userId":"12453935234529602211"},"user_tz":-120},"id":"xRhxooZNV_c9","outputId":"eef26bf3-a7bd-4103-b71c-ff29dd3e8d4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading config file: /usr/local/lib/python3.7/dist-packages/deepforest/data/deepforest_config.yml\n","Loading saved model\n"]},{"ename":"OSError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-d025ef53ae0d\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Cargar modelo entrenado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepforest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Model/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#PRREDICCION CON EL NUEVO MODELO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepforest/deepforest.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights, saved_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 82\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepforest/keras_retinanet/models/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, backbone_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'Model/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}],"source":["modelName='model'\n","# Cargar modelo entrenado\n","model = deepforest.deepforest(saved_model='Model/'+modelName+'.h5')\n","\n","#PRREDICCION CON EL NUEVO MODELO\n","trained_model_boxes = model.predict_tile(\"Orthophoto_University_Example.tif\", return_plot=True)\n","\n","#VISUALIZACION RESULTADOS, CLARAMENTE MEJORA RESPECTO AL MODELO DEFAULT, PARA SOLO TENER 1500 INSTANCIAS Y 1 EPOCA ESTA MUY BIEN\n","fig = plt.figure(figsize=(20,20))\n","plt.imshow(trained_model_boxes)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"elapsed":225,"status":"error","timestamp":1655279607470,"user":{"displayName":"LAZARO FORNIS HERRANZ","userId":"12453935234529602211"},"user_tz":-120},"id":"MwbLMBAoDP7v","outputId":"2769d8dc-a667-4792-f325-e794395ed3fc"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-10-1574bc4845c6\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predicción\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mtrained_model_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Orthophoto_University_Example.tif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create SHP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdeepForestToSHP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIFName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Orthophoto_University_Example\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHPOutputName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Example/net_detections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["# Predicción\n","trained_model_boxes = model.predict_tile(\"Orthophoto_University_Example.tif\", return_plot=False, patch_overlap=0.2,iou_threshold=0.2)\n","# Create SHP\n","deepForestToSHP(trained_model_boxes, TIFName=\"Orthophoto_University_Example\", SHPOutputName='Example/net_detections')\n","# Create CSV\n","SHPtoCSV(SHPName='Example/net_detections', TIFName=\"Orthophoto_University_Example\",CSVName='Example/net_detections_CSV_annotations')"]},{"cell_type":"markdown","metadata":{"id":"WZgFfoFVEAz7"},"source":["# Analysis of the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzlLxDPx5MPt"},"outputs":[],"source":["\n","def calculatePolygonArea(polygonVec):\n","  geod = Geod(ellps=\"WGS84\") # specify a named ellipsoid\n","  poly = Polygon(polygonVec)\n","  area = abs(geod.geometry_area_perimeter(poly)[0])\n","  print('# Geodesic area: {:.3f} m^2'.format(area))\n","  return area\n","\n","def calculateSHPAreas(SHPName, TIFName, CSVName, SHPExtension=\".shp\", TIFExtension=\".tif\", CSVExtension=\".csv\"):\n","  SHP_ESPG = SHPgetESPG(SHPName, SHPExtension)\n","  polyList = SHPgetPolygons(SHPName, SHPExtension)\n","\n","  arrArea = []\n","  for poly in polyList:\n","    a = calculatePolygonArea(poly)\n","    arrArea.append(a)\n","  \n","  return arrArea"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Eh1FN3wGIzb"},"outputs":[],"source":["\n","raw_symbols = SymbolValidator().values\n","\n","symbols = []\n","for i in range(0,len(raw_symbols),3):\n","    symbols.append(raw_symbols[i])\n","colors = px.colors.qualitative.Dark24"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":658},"executionInfo":{"elapsed":245,"status":"error","timestamp":1655279595374,"user":{"displayName":"LAZARO FORNIS HERRANZ","userId":"12453935234529602211"},"user_tz":-120},"id":"kw4LcE2REAM-","outputId":"1293d81a-a702-4081-b598-cf2eebd17bac"},"outputs":[{"name":"stdout","output_type":"stream","text":["SHPtoCSV\n"]},{"ename":"DriverError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n","\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n","\u001b[0;31mCPLE_OpenFailedError\u001b[0m: Example/Test_NoBuildings_NoRoads.shp: No such file or directory","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-5d56fd2e1631\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m \u001b[0mSHPtoCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHPName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Example/Test_NoBuildings_NoRoads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIFName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Orthophoto_University_Example\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCSVName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Example/Test_NoBuildings_NoRoads_CSV_annotations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mshp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Example/Test_NoBuildings_NoRoads\"\u001b[0m \u001b[0;31m# LiDAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-4-be64aeffadcf\u003e\u001b[0m in \u001b[0;36mSHPtoCSV\u001b[0;34m(SHPName, TIFName, CSVName, SHPExtension, TIFExtension, CSVExtension)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SHPtoCSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0;31m#open polygon layer and create list of Polygons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 297\u001b[0;31m   \u001b[0mSHP_ESPG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSHPgetESPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHPName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHPExtension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m   \u001b[0mpolyList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSHPgetPolygons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHPName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHPExtension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolyList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-4-be64aeffadcf\u003e\u001b[0m in \u001b[0;36mSHPgetESPG\u001b[0;34m(SHPName, SHPExtension)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m#################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mSHPgetESPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHPName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHPExtension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".shp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 146\u001b[0;31m   \u001b[0mpolyShp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHPName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mSHPExtension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0mepsgSHP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolyShp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mepsgSHP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsgSHP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsg:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Credentialized: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 417\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--\u003e 265\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n","\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n","\u001b[0;31mDriverError\u001b[0m: Example/Test_NoBuildings_NoRoads.shp: No such file or directory"]}],"source":["############################\n","# Extraer las dimensiones medias de cada bounding-box\n","\n","# Create CSV\n","SHPtoCSV(SHPName='Example/Test_NoBuildings_NoRoads', TIFName=\"Orthophoto_University_Example\",CSVName='Example/Test_NoBuildings_NoRoads_CSV_annotations')\n","\n","shp_test = \"Example/Test_NoBuildings_NoRoads\" # LiDAR\n","csv_test = \"Example/Test_NoBuildings_NoRoads_CSV_annotations\"\n","tif_image = \"Orthophoto_University_Example\"\n","lidarAreas = calculateSHPAreas(shp_test, tif_image, csv_test)\n","\n","shp_result = \"Example/net_detections\"\n","csv_result = \"Example/net_detections_CSV_annotations\"\n","imageAreas = calculateSHPAreas(shp_result, tif_image, csv_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKaAE2TMHLxM"},"outputs":[],"source":["colors = px.colors.qualitative.T10\n","\n","############## CADA SEGMENTATOR POR SEPARADO\n","fig = go.Figure()\n","\n","#aa = [\"LiDAR\"] * len(a)\n","fig.add_trace(\n","    go.Box(\n","        name=\"LiDAR\",\n","        x=lidarAreas,\n","        #boxmean='sd', # True represent mean; 'sd' represent mean and standard deviation\n","    )\n",")\n","#aa = [\"Imagery\"] * len(a)\n","fig.add_trace(\n","    go.Box(\n","        name=\"Imagery\",\n","        x=imageAreas,\n","        #boxmean='sd', # True represent mean; 'sd' represent mean and standard deviation\n","    )\n",")\n","\n","fig.update_layout(\n","    legend=dict(\n","        traceorder=\"normal\",\n","        font=dict(\n","            size=14,\n","            color=\"black\"\n","        ),\n","        bordercolor=\"Black\",\n","        borderwidth=1,\n","        xanchor = \"center\", # Para centrarla horizontalmente\n","        x = 0.5,            # Para centrarla horizontalmente\n","        orientation=\"h\",    # Para centrarla horizontalmente\n","    ),\n","    autosize=False,\n","    width=1400,\n","    height=450,\n","    # figure layout adjustments\n","    #boxmode='group',\n","    #xaxis_tickangle=0,\n",")\n","fig.update_xaxes(range=[0, 100])\n","fig.show() # Para que salga el resultado\n","\n","outputFile         = F\"./percentOverlap.svg\" # svg                   # output file\n","pio.write_image(fig, outputFile) # Se guarda el diagrama en la carpeta de Drive / local"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Script.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}